---
layout: post
title:  "一些hive语句"
date:   2016-07-13 09:05:31
categories: hive
tags: hive
---

* content
{:toc}

## python 部分载入数据

```python

def test_page():
    max_userid = 111
    page = 1
    page_num = 10

    while True:
        start = (page - 1) * page_num + 1
        end = page * page_num
        print start, end
        # process your business
        if end > max_userid:
            break
        page += 1

```

## 对字符串的子集分组

```python

select count(*), substr(reg_time, 1, 7) from smart_analysis.users group by substr(reg_time, 1, 7)

```

## limit offset 的实现

```python

select * from (
    select
    user_phone, userid, last_boot_time, last_order_time, row_number() over (order by userid)  as rowid
    from smart_analysis.users
) T
where rowid >= 10 and rowid <=40

```

## group by topN

```python

SELECT r.app, v.video_id, v.name, r.n FROM vmis.fv_video v
RIGHT OUTER JOIN
(
select app, vid, n
from
    (
    SELECT app, vid, count(*) n, rank() over(PARTITION BY app order by count(*) desc) as top
    FROM gotyou2.fbuffer
    WHERE year = '2017' and month = '08' and day = '16'
    and ok = 0
    GROUP BY app, vid
    ) a
WHERE top < 100
) r
ON v.video_id = r.vid
ORDER BY r.app, r.n DESC
;
```


## udf

```python

import sys
import urllib

for line in sys.stdin:
    app, stp = line.strip('\n').split('\t')
    stp = urllib.unquote(stp)
    print "%s\t%s" % (app, stp)

add file /home/hadoop/tst/udf.py;

select TRANSFORM (app, stp) USING 'python udf.py' AS (app1, stp1) from gotyou2.topic_click where year = '2017' and month = '08' and day = '13' and hour = "00" limit 10;

# hdfs上的路径
add file hdfs:///udf/app_stp.py;

select x.app1, x.stp1, count(*) from
(
select TRANSFORM (app, stp) USING 'python app_stp.py' AS (app1, stp1) from gotyou2.topic_click where year = '2017' and month = '08' and day = '13' and hour = "00") x
group by x.app1, x.stp1;
```

## sqoop

```python

wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz

export SQOOP_HOME=/usr/local/sqoop-1.4.6
export PATH=$SQOOP_HOME/bin:$PATH

# 创建hive表 导入数据到hive表
sqoop import --connect jdbc:mysql://10.1.x.x/vmis --username vmis_read_only  --password xxx --table  fv_channel --hive-import  --warehouse-dir /home/hadoop/hive/warehouse --create-hive-table --hive-database vmis

# 重新导入数据 覆盖 --hive-overwrite
sqoop import --connect jdbc:mysql://10.1.x.x/vmis --username vmis_read_only  --password xx --table  fv_channel --hive-import  --warehouse-dir /home/hadoop/hive/warehouse  --hive-database vmis --hive-overwrite


# 增量导入数据
sqoop job --create um_appuser -- import --connect jdbc:mysql://192.168.1.1:3306/radius --username root --password 123 --table um_appuser --hive-import --hive-table um_appuser --incremental append --check-column id --last-value 15902
```

## 使用第三方包

```python

hadoop@recommended.prod.ctc:~/tst>$ ll software
total 12
drwxr-xr-x 10 hadoop admin 4096 Aug 18 13:51 Crypto
-rw-r--r--  1 hadoop admin 1492 Aug 18 14:43 software_flat.py
-rw-r--r--  1 hadoop admin 1595 Aug 18 14:35 software.py

hive -e "add file software;select TRANSFORM (fudid, app, list, use) USING 'python software/software_flat.py' AS (fudid, app, list, use) from gotyou2.software where year = '2017' and month = '08' and day = '13' and hour = '12' limit 10"

```

## 排重

```python

insert overwrite table store
  select t.p_key,t.sort_word from
    ( select p_key,
           sort_word ,
           row_number()over(distribute by p_key sort by sort_word) as rn
     from store) t
     where t.rn=1;
```

## 分位数

```python

select percentile(cast(priority as BIGINT), array(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0))
from gotyou2.topic_click
where year='2017' and month='09' and day = '10'
and  app = 'aphone_v_smart'
and page = 'play'
;
```


## array插入

```python

CREATE TABLE gotyou2.tmp_test (
fudid STRING,
hours ARRAY<STRING>
);


insert overwrite table gotyou2.tmp_test
select fudid, collect_list(hour) hours
from gotyou2.fbuffer
where year = '2017' and month = '11' and day = '12' and hour = '01'
group by fudid;

```


## 导入csv

```
CREATE TABLE test(a string, b string,..)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   "separatorChar" = ",",
   "quoteChar"     = "\""
)
STORED AS TEXTFILE
location 'location of csv file';
```
